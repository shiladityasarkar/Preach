{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import emojis\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora, models\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import CoherenceModel\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "b5cI8E4Wn93j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2v6n-aaob86",
        "outputId": "ca08ec12-2148-4a2b-b3c7-47f5d8bf0e31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text1(text):#for keyword generation\n",
        "    # Remove emojis\n",
        "    text = emojis.decode(text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove punctuation, stopwords, and non-alphabetic words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation and word.isalpha()]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n"
      ],
      "metadata": {
        "id": "JQ0Y0O2Ppwp7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def generate_keywords_standard(summary, num_keywords=6):\n",
        "    print(\"Generating Keywords...\")\n",
        "\n",
        "    # Tokenize the summary into words\n",
        "    words = preprocess_text1(summary)\n",
        "    # Join the preprocessed words back into a string\n",
        "    processed_text = \" \".join(words)\n",
        "\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit the vectorizer on the summary\n",
        "    vectorizer.fit([processed_text])\n",
        "\n",
        "    # Transform the summary into TF-IDF matrix\n",
        "    tfidf_matrix = vectorizer.transform([processed_text])\n",
        "\n",
        "    # Extract feature names (words)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Compute TF-IDF scores for each word\n",
        "    scores = tfidf_matrix.toarray().flatten()\n",
        "\n",
        "    # Sort the words based on TF-IDF scores\n",
        "    keywords = [feature_names[i] for i in scores.argsort()[::-1][:num_keywords]]\n",
        "\n",
        "    return keywords\n"
      ],
      "metadata": {
        "id": "cyYJ9Wsrpws3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "\n",
        "def main():\n",
        "    # Sample big paragraph\n",
        "    paragraph = \"\"\"\n",
        "    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed non risus. Suspendisse lectus tortor, dignissim sit amet, adipiscing nec, ultricies sed, dolor. Cras elementum ultrices diam. Maecenas ligula massa, varius a, semper congue, euismod non, mi. Proin porttitor, orci nec nonummy molestie, enim est eleifend mi, non fermentum diam nisl sit amet erat. Duis semper. Duis arcu massa, scelerisque vitae, consequat in, pretium a, enim. Pellentesque congue. Ut in risus volutpat libero pharetra tempor. Cras vestibulum bibendum augue. Praesent egestas leo in pede. Praesent blandit odio eu enim. Pellentesque sed dui ut augue blandit sodales. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Aliquam nibh. Mauris ac mauris sed pede pellentesque fermentum. Maecenas adipiscing ante non diam sodales...\n",
        "    \"\"\"\n",
        "\n",
        "    # Start measuring time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generate keywords\n",
        "    keywords = generate_keywords_standard(paragraph)\n",
        "\n",
        "    # End measuring time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    # Print time taken\n",
        "    print(\"Time taken:\", time_taken, \"seconds\")\n",
        "\n",
        "    # Print CPU usage\n",
        "    cpu_usage = psutil.cpu_percent()\n",
        "    print(\"CPU usage:\", cpu_usage, \"%\")\n",
        "\n",
        "    # Print memory usage\n",
        "    memory_usage = psutil.virtual_memory().percent\n",
        "    print(\"Memory usage:\", memory_usage, \"%\")\n",
        "\n",
        "    # Display keywords\n",
        "    print(\"Keywords:\", keywords)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guqcEemVpwv5",
        "outputId": "2f816ad5-fb2f-4319-a634-c96d5e21319e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Keywords...\n",
            "Time taken: 1.6271331310272217 seconds\n",
            "CPU usage: 68.2 %\n",
            "Memory usage: 7.4 %\n",
            "Keywords: ['sed', 'non', 'enim', 'diam', 'adipiscing', 'pellentesque']\n"
          ]
        }
      ]
    }
  ]
}